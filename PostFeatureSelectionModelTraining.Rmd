---
title: "Post Feature Selection Model Training"
author: "Soren Dunn"
date: "2023-11-20"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(xgboost)
library(readr)
library(ggplot2)
```

This file provides some functions for automatically running and evaluating multiple models types on different feature subsets. Simply put the feature sets in the feature_subsets folder and run this code. Results will be outputted to a results csv is the results directory.

```{r}
# Function to convert dataframe to formatted string (which we use to convert dataframe of hyper-parameters into a string)
df_to_string <- function(df) {
  row_values <- as.list(df[1, ])
  formatted_str <- paste(names(row_values), row_values, sep=": ", collapse=", ")
  return(formatted_str)
}

# This first function extracts the accuracies of the best performing set of hyperparameters from the results dataframes
select_best_models <- function(model_dataframes, friendly_model_names) {
  print("got into selecting best models")
  results <- do.call(rbind, lapply(model_dataframes, function(df) {
    # Find the row with the highest accuracy
    best_index <- which.max(df$Accuracy)
    
    # Select the row with the highest accuracy
    best_row <- df[best_index, ]
    
    # Assuming all other columns are hyperparameters
    hyper_params <- best_row[names(best_row) != "Accuracy" & names(best_row) != "Kappa" &
                             names(best_row) != "AccuracySD" & names(best_row) != "KappaSD"]

    # Create a summary dataframe containing the best values and corresponding hyperparameters
    summary <- data.frame(
      BestAccuracy = best_row$Accuracy,
      BestKappa = best_row$Kappa,
      BestAccuracySD = best_row$AccuracySD,
      BestKappaSD = best_row$KappaSD,
      HyperParameters = df_to_string(hyper_params)
    )

    return(summary)
  }))
  
  # Adding an identifier for each model (assuming each dataframe corresponds to one model)
  results$Model <- unlist(friendly_model_names)
  print("successfully got results")
  # Return the results
  return(results)
}
```

```{r}
# Helper Function to Generate Model File Name
generate_model_file_name <- function(dataset_name, model_type, train_control) {
  folder_name <- "trained_models"
  dir.create(folder_name, showWarnings = FALSE)  # Create the directory if it doesn't exist, suppress warnings if it does
  file_name <- paste0(dataset_name, "_", model_type, "_", train_control$method, "_", train_control$number, "foldCV.RData")
  return(file.path(folder_name, file_name))  # Return the full file path within the 'trained_models' folder
}

# Define the function to train and evaluate models
train_and_evaluate_models <- function(df, outcome_var_name, dataset_name, existing_results_df=NULL) {
  # Set up control parameters for model training
  train_control <- trainControl(method = "cv", number = 5) # 5-fold cross-validation
  
  # Initialize a list to hold our models
  trained_models <- list()
  trained_model_results <- list()

  # Define model types to consider
  model_types <- c("multinom", "xgbTree") # Add "rf" for random forest model or tags for other models as desired
  
  # Map friendly names to model_types for output purposes
  friendly_model_names <- c("LogisticRegression", "XGBoost") # Update with "RandomForest" or other desired models are added
  untrained_model_names <- c()
  model_file_names <- c()
  
  # Loop through each model type
  for (i in seq_along(model_types)) {
    model_type <- model_types[i]
    friendly_name <- friendly_model_names[i]
    
    # Generate a file name for saving/loading the model
    model_file_name <- generate_model_file_name(dataset_name, model_type, train_control)
    model_file_names <- c(model_file_names, model_file_name)
    
    # Check if the model has already been trained and saved
    if (file.exists(model_file_name)) {
      cat("Loading saved model from: ", model_file_name, "\n")
      model <- readRDS(model_file_name)
    } else {
      # Train the model if it hasn't been saved
      set.seed(123) # For reproducibility
      model <- train(reformulate(".", response = outcome_var_name),
                     data = df,
                     method = model_type,
                     trControl = train_control)
      
      # Save the trained model
      cat("Saving trained model to: ", model_file_name, "\n")
      saveRDS(model, model_file_name)
    }
    
    print("got past checking if file exists")

    # Check if this model's results already exist in the provided results dataframe
    if (!is.null(existing_results_df)) {
      model_hyper_params <- model$bestTune  # Assuming that bestTune contains the best hyperparameters for the model
      # Check if the combination of model_type and hyperparameters already exists in the existing_results_df
      # Check if the combination of model_type and hyperparameters already exists in the existing_results_df
      # By creating logical vectors for model match and hyperparameters match
      print("got to model match")
      model_match <- existing_results_df$Model == model_file_name
      print("got past model match")
      # Assuming df_to_string is a function to convert dataframe columns to string for comparison.
      # Please ensure that df_to_string and HyperParameters are appropriately defined as this snippet assumes their existence/format.
      hyperparam_match <- existing_results_df$HyperParameters == df_to_string(model_hyper_params)
      print("got past hyperparam match")
      # Combine the two logical vectors to see if there is any row that satisfies both conditions
      existing_row <- model_match & hyperparam_match
      print("got past existing_row")

      if (sum(existing_row)) {
        cat("Results for", model_file_name, "with these hyperparameters already exist. Skipping.\n")
        # Skip the current model type and move to the next one
        next
      } else {
        print("adding to untrained model names")
        print(model_file_name)
        untrained_model_names <- c(untrained_model_names, model_file_name)
        print("untrained_model_names")
        print(untrained_model_names)
      }
    } else {
      untrained_model_names <- model_file_names
    }
    
    # Add the trained model to our list
    trained_models[[friendly_name]] <- model
    trained_model_results <- append(trained_model_results,list(model$results))
  }

  new_results <- select_best_models(trained_model_results, untrained_model_names)

  # Finally, if existing_results_df is provided, append the new models' best results
  if (!is.null(existing_results_df)) {
    combined_results <- rbind(existing_results_df, new_results)
    return(list(models = trained_models, results = combined_results))
  }
  print("successfully got models")
  return(list(models = trained_models, results = new_results))
}

# To call the function, provide the dataframe (df), the outcome variable name (outcome_var_name),
# the name of the dataset (dataset_name), and optionally the existing results dataframe (existing_results_df).
# Example:
# models <- train_and_evaluate_models(my_dataframe, "Survived", "titanic", existing_results_df=previous_results)
```

You can save all the feature subset csv's to the feature_subsets directory and run the following code to automatically evaluate all the models on these different feature subsets. If a model has already been run for a particular dataset, then the previously run model will be loaded in instead of retraining it. The data is added to results.csv in the results directory.

```{r}
# Define the directory where CSV dataframes and results will be stored
path_to_csv_directory <- "feature_subsets"
results_directory <- "results"

# Create the 'results' directory if it does not exist
dir.create(results_directory, showWarnings = FALSE)

# List all CSV files in the directory
csv_files <- list.files(path_to_csv_directory, pattern = "\\.csv$", full.names = TRUE)

# Loop over each CSV file, train and evaluate models
for (csv_file in csv_files) {
  # Extract the dataset name from the file name
  dataset_name <- tools::file_path_sans_ext(basename(csv_file))
  results_file_path <- file.path(results_directory, "results.csv")
  
  # Load existing results if the file exists
  if (file.exists(results_file_path)) {
    existing_results <- read.csv(results_file_path)
  } else {
    existing_results <- NULL
  }
  
  # Read the dataset from the CSV file
  df <- read.csv(csv_file)
  
  # Check if 'project_id' column exists in the dataframe
  if (!"project_id" %in% names(df)) {
    stop("The outcome variable 'project_id' does not exist in the dataframe: ", dataset_name)
  }
  
  # Train and evaluate the models, passing in existing results if available
  trained_models_and_results <- train_and_evaluate_models(df, "project_id", dataset_name, existing_results)
  
  # Now save the updated results to the 'results' directory
  if (!is.null(trained_models_and_results$results)) {
    write.csv(trained_models_and_results$results, results_file_path, row.names = FALSE)
  }
}

# At this point, each dataset's results have been saved in the 'results' directory as separate CSV files.
```

```{r}
# Do the same for the testing data
# Define the directory where CSV dataframes and results will be stored
path_to_csv_directory <- "testing_csvs"
results_directory <- "results"

# Create the 'results' directory if it does not exist
dir.create(results_directory, showWarnings = FALSE)

# List all CSV files in the directory
csv_files <- list.files(path_to_csv_directory, pattern = "\\.csv$", full.names = TRUE)

# Loop over each CSV file, train and evaluate models
for (csv_file in csv_files) {
  # Extract the dataset name from the file name
  dataset_name <- tools::file_path_sans_ext(basename(csv_file))
  results_file_path <- file.path(results_directory, "test_results.csv")
  
  # Load existing results if the file exists
  if (file.exists(results_file_path)) {
    existing_results <- read.csv(results_file_path)
  } else {
    existing_results <- NULL
  }
  
  # Read the dataset from the CSV file
  df <- read.csv(csv_file)
  
  # Check if 'project_id' column exists in the dataframe
  if (!"project_id" %in% names(df)) {
    stop("The outcome variable 'project_id' does not exist in the dataframe: ", dataset_name)
  }
  
  # Train and evaluate the models, passing in existing results if available
  trained_models_and_results <- train_and_evaluate_models(df, "project_id", dataset_name, existing_results)
  
  # Now save the updated results to the 'results' directory
  if (!is.null(trained_models_and_results$results)) {
    write.csv(trained_models_and_results$results, results_file_path, row.names = FALSE)
  }
}
```